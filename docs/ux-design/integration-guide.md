# Integration Guide: Implementing Prompt-First Notion Design

## üöÄ Quick Start Guide

This guide provides step-by-step instructions for integrating the revolutionary prompt-first design into your knowledge pipeline.

## üìã Prerequisites

- Notion API access
- Python 3.8+
- Understanding of the current `notion_formatter.py` structure
- Access to prompt configuration database

## üîß Implementation Steps

### Step 1: Extend the Current Notion Formatter

```python
# Update src/utils/notion_formatter.py

from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime
from enum import Enum

class EnhancedNotionFormatter(NotionFormatter):
    """Enhanced formatter with prompt attribution and visual hierarchy."""
    
    def __init__(self):
        super().__init__()
        self.prompt_attribution = PromptAttributionFormatter()
        
    def format_with_attribution(
        self, 
        analyses: Dict[str, Any],
        source_metadata: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """Format content with full prompt attribution."""
        
        blocks = []
        
        # 1. Add Executive Dashboard
        blocks.extend(self.prompt_attribution.create_executive_dashboard(analyses))
        
        # 2. Add each analysis section with attribution
        for analyzer_name, analysis_data in analyses.items():
            if 'prompt_metadata' in analysis_data:
                blocks.extend(
                    self.prompt_attribution.create_attributed_content_section(
                        section_title=analysis_data.get('section_title', analyzer_name),
                        prompt_name=analysis_data['prompt_metadata']['prompt_name'],
                        prompt_id=analysis_data['prompt_metadata']['prompt_id'],
                        content=analysis_data,
                        metadata=analysis_data['prompt_metadata']
                    )
                )
        
        return blocks
```

### Step 2: Update Analyzer Base Class

```python
# Update src/enrichment/base_analyzer.py

class BaseAnalyzer(ABC):
    """Enhanced base analyzer with prompt tracking."""
    
    def analyze(self, content: str, title: str, content_type: Optional[str] = None) -> Dict[str, Any]:
        """Analyze with prompt attribution."""
        
        # Get prompt configuration
        prompt_config = self.prompt_config.get_prompt(self.analyzer_name, content_type)
        prompt_metadata = {
            'prompt_name': prompt_config.get('name', f'{self.analyzer_name}_default'),
            'prompt_id': prompt_config.get('id', 'unknown'),
            'prompt_version': prompt_config.get('version', '1.0'),
            'timestamp': datetime.now().isoformat()
        }
        
        # Perform analysis (existing code)
        result = self._perform_analysis(content, title, prompt_config)
        
        # Add prompt metadata to result
        result['prompt_metadata'] = prompt_metadata
        result['prompt_metadata']['quality_score'] = self._calculate_quality_score(result)
        result['prompt_metadata']['model'] = self.config.openai.model
        
        # Extract structured insights
        result['insights'] = self._extract_structured_insights(result['analysis'])
        result['section_title'] = self._generate_section_title(self.analyzer_name, content_type)
        
        return result
    
    def _extract_structured_insights(self, analysis_text: str) -> List[Dict[str, Any]]:
        """Extract structured insights from analysis text."""
        insights = []
        
        # Use NLP or pattern matching to extract insights
        # This is a simplified example
        patterns = {
            'opportunity': r'opportunity|potential|growth|expansion',
            'risk': r'risk|threat|challenge|concern',
            'action': r'should|must|recommend|action|implement',
            'data': r'\d+%|\$\d+|statistics|data|metrics'
        }
        
        # Extract and structure insights
        # (Implementation depends on your specific needs)
        
        return insights
    
    def _calculate_quality_score(self, result: Dict[str, Any]) -> int:
        """Calculate quality score for the analysis."""
        score = 100
        
        # Deduct points for various issues
        if not result.get('analysis'):
            score -= 50
        if result.get('error'):
            score -= 30
        if len(result.get('analysis', '')) < 100:
            score -= 20
        if not result.get('web_search_used') and result.get('web_search_requested'):
            score -= 10
            
        return max(0, score)
```

### Step 3: Create Prompt Database Integration

```python
# New file: src/core/prompt_database.py

class PromptDatabase:
    """Manages prompt configurations and metadata."""
    
    def __init__(self, notion_client):
        self.client = notion_client
        self.prompts_db_id = os.getenv('NOTION_PROMPTS_DB_ID')
        self._cache = {}
    
    def get_prompt_metadata(self, prompt_name: str) -> Dict[str, Any]:
        """Get prompt metadata from Notion database."""
        
        if prompt_name in self._cache:
            return self._cache[prompt_name]
        
        # Query Notion for prompt configuration
        results = self.client.databases.query(
            database_id=self.prompts_db_id,
            filter={
                "property": "Name",
                "title": {"equals": prompt_name}
            }
        )
        
        if results['results']:
            page = results['results'][0]
            metadata = {
                'prompt_id': page['id'],
                'prompt_name': prompt_name,
                'version': self._extract_property(page, 'Version'),
                'category': self._extract_property(page, 'Category'),
                'description': self._extract_property(page, 'Description'),
                'performance_metrics': {
                    'avg_quality_score': self._extract_property(page, 'Avg Quality Score'),
                    'usage_count': self._extract_property(page, 'Usage Count'),
                    'last_updated': self._extract_property(page, 'Last Updated')
                }
            }
            self._cache[prompt_name] = metadata
            return metadata
        
        return {
            'prompt_id': 'unknown',
            'prompt_name': prompt_name,
            'version': '1.0'
        }
    
    def update_prompt_metrics(self, prompt_name: str, quality_score: int):
        """Update prompt performance metrics."""
        # Update usage count and rolling average quality score
        pass
```

### Step 4: Update Pipeline Processor

```python
# Update src/core/processor.py

class PipelineProcessor:
    """Enhanced processor with prompt attribution."""
    
    def _enrich_content(self, source: SourceContent) -> Dict[str, Any]:
        """Enrich content with multiple analyzers."""
        
        analyses = {}
        
        for analyzer_name, analyzer in self.analyzers.items():
            try:
                # Analyze with prompt attribution
                result = analyzer.analyze(
                    content=source.content,
                    title=source.title,
                    content_type=source.content_type
                )
                
                # Add analyzer name for tracking
                result['analyzer_name'] = analyzer_name
                analyses[analyzer_name] = result
                
            except Exception as e:
                self.logger.error(f"Analysis failed for {analyzer_name}: {e}")
                analyses[analyzer_name] = {
                    'analysis': f"Analysis failed: {str(e)}",
                    'error': True,
                    'prompt_metadata': {
                        'quality_score': 0,
                        'error_message': str(e)
                    }
                }
        
        return self._format_enriched_content(analyses, source)
    
    def _format_enriched_content(self, analyses: Dict[str, Any], source: SourceContent) -> List[Dict[str, Any]]:
        """Format enriched content with new design."""
        
        formatter = EnhancedNotionFormatter()
        
        # Prepare data structure
        formatted_analyses = {
            'source_metadata': {
                'title': source.title,
                'url': source.url,
                'type': source.content_type,
                'processed_at': datetime.now().isoformat()
            },
            'analyses': analyses
        }
        
        return formatter.format_with_attribution(formatted_analyses, source)
```

### Step 5: Mobile Detection and Responsive Formatting

```python
# Add to src/utils/notion_formatter.py

class ResponsiveNotionFormatter:
    """Handles responsive formatting based on device context."""
    
    def __init__(self, is_mobile: bool = False):
        self.is_mobile = is_mobile
    
    def format_insight_card(self, insight: Dict[str, Any]) -> Dict[str, Any]:
        """Format insight as card optimized for device."""
        
        if self.is_mobile:
            # Mobile-optimized card
            return {
                "type": "callout",
                "callout": {
                    "icon": {"emoji": insight['emoji']},
                    "color": insight['color'],
                    "rich_text": [
                        {
                            "text": {"content": insight['title']},
                            "annotations": {"bold": True}
                        },
                        {
                            "text": {"content": f"\n\n{self._truncate(insight['summary'], 150)}"}
                        },
                        {
                            "text": {
                                "content": f"\n\nTap for details ‚Üí"
                            },
                            "annotations": {"italic": True, "color": "gray"}
                        }
                    ]
                }
            }
        else:
            # Desktop full card
            return self._create_full_insight_card(insight)
```

### Step 6: Configuration Updates

```yaml
# Update config/prompts.yaml

prompts:
  market:
    default:
      name: "Market_News_Insights_v2.1"
      id: "prompt-mkt-001"
      version: "2.1"
      system: |
        You are a market analysis expert. Focus on actionable insights.
        Structure your response with clear sections:
        - Executive Summary (3 key points)
        - Opportunities (with impact and timeline)
        - Risks (with severity and mitigation)
        - Action Items (specific and measurable)
      temperature: 0.3
      web_search: true
      
  technical:
    default:
      name: "Technical_Analyzer_v3.0"
      id: "prompt-tech-001"
      version: "3.0"
      system: |
        You are a technical analysis expert. Provide deep technical insights.
        Structure your response with:
        - Technical Summary
        - Key Technical Findings
        - Implementation Recommendations
        - Technical Risks and Mitigations
```

## üéØ Testing & Validation

### Unit Tests for New Components

```python
# tests/test_prompt_attribution.py

import pytest
from src.utils.notion_formatter import PromptAttributionFormatter

class TestPromptAttribution:
    def test_executive_dashboard_creation(self):
        formatter = PromptAttributionFormatter()
        sample_data = {
            'analyses': {
                'market': {
                    'insights': [
                        {'type': 'action', 'title': 'Test Action'},
                        {'type': 'risk', 'title': 'Test Risk'}
                    ]
                }
            }
        }
        
        blocks = formatter.create_executive_dashboard(sample_data)
        assert len(blocks) > 0
        assert any(block['type'] == 'heading_1' for block in blocks)
        assert any(block['type'] == 'column_list' for block in blocks)
    
    def test_quality_score_indicator(self):
        formatter = PromptAttributionFormatter()
        assert formatter._get_quality_indicator(95) == "‚úÖ"
        assert formatter._get_quality_indicator(75) == "‚ö†Ô∏è"
        assert formatter._get_quality_indicator(50) == "‚ùå"
```

### Integration Tests

```python
# tests/test_integration.py

def test_full_pipeline_with_attribution():
    """Test the complete pipeline with prompt attribution."""
    
    # Create test content
    test_content = SourceContent(
        title="AI Market Analysis",
        content="Latest AI market trends...",
        url="https://example.com/ai-analysis"
    )
    
    # Process through pipeline
    processor = PipelineProcessor(config)
    notion_blocks = processor.process_single(test_content)
    
    # Verify structure
    assert any("Executive Intelligence Dashboard" in str(block) for block in notion_blocks)
    assert any("Generated by:" in str(block) for block in notion_blocks)
    assert any("Quality Score:" in str(block) for block in notion_blocks)
```

## üìä Performance Monitoring

### Metrics to Track

1. **Quality Scores by Prompt**
   - Average quality score per prompt
   - Quality score trends over time
   - Failure rates by prompt

2. **User Engagement**
   - Toggle interaction rates
   - Time to find key insights
   - Mobile vs desktop usage

3. **Content Performance**
   - Most viewed sections
   - Action item completion rates
   - Insight usefulness ratings

### Implementation

```python
# src/monitoring/metrics.py

class NotionMetricsTracker:
    """Track metrics for the new design."""
    
    def track_prompt_performance(self, prompt_name: str, quality_score: int):
        """Track prompt performance metrics."""
        # Log to analytics system
        pass
    
    def track_user_interaction(self, interaction_type: str, section: str):
        """Track user interactions with content."""
        # Log toggle opens, card clicks, etc.
        pass
```

## üöÄ Deployment Checklist

- [ ] Update all analyzer classes with prompt metadata
- [ ] Create Notion prompts database with configurations
- [ ] Implement quality scoring algorithm
- [ ] Add structured insight extraction
- [ ] Update formatter with attribution components
- [ ] Test mobile responsive formatting
- [ ] Configure web search attribution display
- [ ] Set up performance monitoring
- [ ] Create user documentation
- [ ] Plan phased rollout

## üéØ Success Criteria

1. **100% Prompt Attribution**: Every content section shows its generating prompt
2. **70% Faster Insight Discovery**: Users find key insights in <30 seconds
3. **Mobile Usage >40%**: Significant mobile engagement
4. **Quality Score >85%**: Average quality across all prompts
5. **User Satisfaction >4.5/5**: Based on feedback surveys

## üìö Additional Resources

- [Notion API Documentation](https://developers.notion.com/)
- [Design System Guidelines](./notion-content-redesign.md)
- [Visual Wireframes](./visual-wireframes.md)
- [Implementation Code](./implementation-code.py)

## ü§ù Support

For questions or issues with implementation:
- Review the design documentation
- Check the implementation examples
- Consult the visual wireframes
- Test with sample data first

Remember: The goal is to transform walls of text into scannable, actionable intelligence with complete prompt transparency!
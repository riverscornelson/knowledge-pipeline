# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Pipeline Architecture

This is a **knowledge pipeline** that ingests content from multiple sources (Google Drive PDFs, websites, Gmail) into a Notion database, then enriches it with AI-generated summaries and classifications.

### Core Processing Flow

1. **Ingestion**: Multiple capture scripts add content to Notion "Sources" database with Status="Inbox"
2. **Enrichment**: AI processing scripts generate summaries, classifications, and analysis
3. **Storage**: Results stored as Notion page properties + toggle blocks with detailed analysis

### Production Pipeline (v2.0)

**ðŸš€ Knowledge Pipeline v2.0** (`scripts/run_pipeline.py`) - **PRODUCTION**:
- **Modular architecture** with organized package structure under `src/`
- **Priority-based processing** with Google Drive as primary source
- **75% faster processing** with streamlined AI analysis
- **Proper Python packaging** with `pip install -e .` installation
- **Centralized configuration** via `PipelineConfig.from_env()`


## Core Notion Database Schema

All scripts work with a **Sources** database containing these critical properties:
- **Status**: Select (`Inbox` â†’ `Enriched`/`Failed`) - controls processing workflow
- **Title**: Title - primary content identifier
- **Drive URL**: URL - for PDF sources from Google Drive
- **Article URL**: URL - for website/email sources  
- **Hash**: Text - SHA-256 for deduplication
- **Content-Type**: Select - AI-classified content category
- **AI-Primitive**: Multi-select - AI capability classification
- **Summary**: Text - brief overview (under 200 chars) generated by enrichment
- **Vendor**: Select - company/organization mentioned
- **Created Date**: Date - original publication date

Classification taxonomies are **dynamically loaded** from the Notion database schema, so updating select options in Notion automatically changes available categories.

## OpenAI Integration Pattern

All AI processing uses a **dual API approach**:
1. **Preferred**: OpenAI Responses API (if available in client version â‰¥1.3)
2. **Fallback**: Chat Completions API for older clients

Models configurable via environment variables (`MODEL_SUMMARY`, `MODEL_CLASSIFIER`, etc.) with `gpt-4.1` as default.

## Key Environment Variables

**Core**:
- `NOTION_TOKEN`, `NOTION_SOURCES_DB` - Notion database access
- `OPENAI_API_KEY` - AI processing
- `GOOGLE_APP_CREDENTIALS` - Google Drive/Gmail access

**Processing Control**:
- `GMAIL_WINDOW_DAYS` (default 7) - Gmail search window
- `WEBSITE_WINDOW_DAYS` (default 30) - website content recency

## Commands

### Basic Operations
```bash
# Install package in development mode
pip install -e .

# Run complete pipeline (all sources + enrichment)
python scripts/run_pipeline.py

# Run specific source only
python scripts/run_pipeline.py --source drive
python scripts/run_pipeline.py --source gmail

# Skip enrichment phase
python scripts/run_pipeline.py --skip-enrichment
```

### Module Structure
```python
# Core modules
from src.core.config import PipelineConfig
from src.core.notion_client import NotionClient
from src.core.models import SourceContent, ContentStatus

# Drive ingestion (primary source)
from src.drive.ingester import DriveIngester
from src.drive.pdf_processor import PDFProcessor

# Enrichment
from src.enrichment.processor import EnrichmentProcessor
from src.enrichment.summarizer import ContentSummarizer
from src.enrichment.classifier import ContentClassifier

# Secondary sources
from src.secondary_sources.gmail.capture import GmailCapture
from src.secondary_sources.firecrawl.capture import FirecrawlCapture
```

### Pipeline Management
```bash
# Monitor pipeline execution
tail -f logs/pipeline.jsonl | jq .

# View errors only
cat logs/pipeline.jsonl | jq 'select(.level == "ERROR")'
```

## Module Architecture (v2.0)

### Primary Source: Google Drive
- `src/drive/ingester.py` - Main ingestion orchestrator
- `src/drive/pdf_processor.py` - PDF text extraction
- `src/drive/deduplication.py` - Content hash management

### Secondary Sources
- `src/secondary_sources/gmail/` - OAuth2 email capture
- `src/secondary_sources/firecrawl/` - Web scraping
- Lower priority than Drive content

### AI Enrichment
- `src/enrichment/processor.py` - Orchestrates all AI analysis
- `src/enrichment/summarizer.py` - Content summarization
- `src/enrichment/classifier.py` - Dynamic taxonomy classification
- `src/enrichment/insights.py` - Actionable insights extraction

## Error Handling Patterns

**Sequential Scripts**: Basic try/catch with status updates to "Failed"

**Enhanced Scripts**: Include:
- Exponential backoff retry logic for API timeouts
- Graceful handling of archived Notion pages
- Structured logging with performance metrics
- Resilient fallbacks when operations fail

## Content Processing Workflow

1. **Ingestion Phase**: 
   - `DriveIngester`, `GmailCapture`, or `FirecrawlCapture` create `SourceContent`
   - Content added to Notion with Status="Inbox"
   - Deduplication via SHA-256 hashing

2. **Enrichment Phase**:
   - `EnrichmentProcessor.process_batch()` fetches Status="Inbox" items
   - Orchestrates AI analysis:
     - `ContentSummarizer` â†’ Full markdown summary
     - `ContentClassifier` â†’ Dynamic taxonomy classification
     - `InsightsGenerator` â†’ Actionable insights
   - Updates Status to "Enriched" or "Failed"

3. **Storage Structure**:
   - **Properties**: Summary (<200 chars), classifications, metadata
   - **Page Body**: Notion blocks with formatted content

## Key v3.0 Improvements

- **Modular Architecture**: Clean separation in `src/` packages
- **Type Safety**: Full type hints throughout codebase
- **Resilience**: Exponential backoff, graceful error handling
- **Performance**: Generator patterns, efficient batch processing
- **Testing**: Organized pytest structure in `tests/`
- **Documentation**: Comprehensive docs in `docs/` directory

## Development Tips

- Always use `PipelineConfig.from_env()` for configuration
- Check existing patterns before implementing new features
- Use structured logging via `setup_logger()`
- Apply `@with_notion_resilience()` for Notion API calls
- Follow existing code style and type hints
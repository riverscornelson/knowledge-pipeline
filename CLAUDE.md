# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Pipeline Architecture

This is a **knowledge pipeline** that ingests content from Google Drive PDFs into a Notion database, then enriches it with AI-generated summaries and classifications.

### Core Processing Flow

1. **Ingestion**: Google Drive ingester adds PDF content to Notion "Sources" database with Status="Inbox"
2. **Enrichment**: AI processing scripts generate summaries, classifications, and analysis
3. **Storage**: Results stored as Notion page properties + toggle blocks with detailed analysis

### Production Pipeline (v3.0)

**ðŸš€ Knowledge Pipeline v3.0** (`scripts/run_pipeline.py`) - **PRODUCTION**:
- **Modular architecture** with organized package structure under `src/`
- **Google Drive focus** - streamlined to core functionality 
- **75% faster processing** with streamlined AI analysis
- **Proper Python packaging** with `pip install -e .` installation
- **Centralized configuration** via `PipelineConfig.from_env()`


## Core Notion Database Schema

All scripts work with a **Sources** database containing these critical properties:
- **Status**: Select (`Inbox` â†’ `Enriched`/`Failed`) - controls processing workflow
- **Title**: Title - primary content identifier
- **Drive URL**: URL - for PDF sources from Google Drive
- **Article URL**: URL - for website/email sources  
- **Hash**: Text - SHA-256 for deduplication
- **Content-Type**: Select - AI-classified content category
- **AI-Primitive**: Multi-select - AI capability classification
- **Summary**: Text - brief overview (under 200 chars) generated by enrichment
- **Vendor**: Select - company/organization mentioned
- **Created Date**: Date - original publication date
- **Topical-Tags**: Multi-select - subject matter and theme tags (3-5 per document)
- **Domain-Tags**: Multi-select - industry and application area tags (2-4 per document)

Classification taxonomies and tag options are **dynamically loaded** from the Notion database schema, so updating select options in Notion automatically changes available categories.

## OpenAI Integration Pattern

All AI processing uses a **dual API approach**:
1. **Preferred**: OpenAI Responses API (if available in client version â‰¥1.3)
2. **Fallback**: Chat Completions API for older clients

Models configurable via environment variables (`MODEL_SUMMARY`, `MODEL_CLASSIFIER`, etc.) with `gpt-4.1` as default.

## Key Environment Variables

**Core**:
- `NOTION_TOKEN`, `NOTION_SOURCES_DB` - Notion database access
- `OPENAI_API_KEY` - AI processing
- `GOOGLE_APP_CREDENTIALS` - Google Drive access

**Processing Control**:
- `BATCH_SIZE` (default 10) - Number of items to process per batch
- `RATE_LIMIT_DELAY` (default 0.3) - Delay between API calls

## Commands

### Basic Operations
```bash
# Install package in development mode
pip install -e .

# Run complete pipeline (ingestion + enrichment)
python scripts/run_pipeline.py

# Skip enrichment phase (ingestion only)
python scripts/run_pipeline.py --skip-enrichment
```

### Module Structure
```python
# Core modules
from src.core.config import PipelineConfig
from src.core.notion_client import NotionClient
from src.core.models import SourceContent, ContentStatus

# Drive ingestion (primary source)
from src.drive.ingester import DriveIngester
from src.drive.pdf_processor import PDFProcessor

# Enrichment
from src.enrichment.processor import EnrichmentProcessor
from src.enrichment.summarizer import ContentSummarizer
from src.enrichment.classifier import ContentClassifier
from src.enrichment.content_tagger import ContentTagger

# Future integrations (see FUTURE_FEATURES.md)
# src.secondary_sources package structure preserved for extensions
```

### Pipeline Management
```bash
# Monitor pipeline execution
tail -f logs/pipeline.jsonl | jq .

# View errors only
cat logs/pipeline.jsonl | jq 'select(.level == "ERROR")'
```

### Content Tagging
```bash
# Backfill tags for existing enriched content
python scripts/backfill_tags.py

# Process limited number with dry run
python scripts/backfill_tags.py --limit 10 --dry-run

# Verbose output for debugging
python scripts/backfill_tags.py --verbose
```

## Module Architecture (v3.0)

### Primary Source: Google Drive
- `src/drive/ingester.py` - Main ingestion orchestrator
- `src/drive/pdf_processor.py` - PDF text extraction
- `src/drive/deduplication.py` - Content hash management

### Future Integrations
- `src/secondary_sources/` - Package structure preserved for future sources
- See `FUTURE_FEATURES.md` for planned Gmail and web content integrations

### AI Enrichment
- `src/enrichment/processor.py` - Orchestrates all AI analysis
- `src/enrichment/summarizer.py` - Content summarization
- `src/enrichment/classifier.py` - Dynamic taxonomy classification
- `src/enrichment/insights.py` - Actionable insights extraction
- `src/enrichment/content_tagger.py` - Intelligent tagging with consistency focus

## Error Handling Patterns

**Sequential Scripts**: Basic try/catch with status updates to "Failed"

**Enhanced Scripts**: Include:
- Exponential backoff retry logic for API timeouts
- Graceful handling of archived Notion pages
- Structured logging with performance metrics
- Resilient fallbacks when operations fail

## Content Processing Workflow

1. **Ingestion Phase**: 
   - `DriveIngester` creates `SourceContent` from Google Drive PDFs
   - Content added to Notion with Status="Inbox"
   - Deduplication via SHA-256 hashing

2. **Enrichment Phase**:
   - `EnrichmentProcessor.process_batch()` fetches Status="Inbox" items
   - Orchestrates AI analysis:
     - `ContentSummarizer` â†’ Full markdown summary
     - `ContentClassifier` â†’ Dynamic taxonomy classification
     - `InsightsGenerator` â†’ Actionable insights
     - `ContentTagger` â†’ Topical and domain tags (prefers existing tags)
   - Updates Status to "Enriched" or "Failed"

3. **Storage Structure**:
   - **Properties**: Summary (<200 chars), classifications, metadata, tags
   - **Page Body**: Notion blocks with formatted content

## Key v3.0 Improvements

- **Modular Architecture**: Clean separation in `src/` packages
- **Type Safety**: Full type hints throughout codebase
- **Resilience**: Exponential backoff, graceful error handling
- **Performance**: Generator patterns, efficient batch processing
- **Testing**: Organized pytest structure in `tests/`
- **Documentation**: Comprehensive docs in `docs/` directory

## Development Tips

- Always use `PipelineConfig.from_env()` for configuration
- Check existing patterns before implementing new features
- Use structured logging via `setup_logger()`
- Apply `@with_notion_resilience()` for Notion API calls
- Follow existing code style and type hints
"""
Prompt Attribution Implementation for Knowledge Pipeline
Provides complete traceability between AI-generated content and source prompts
"""

import json
import uuid
from datetime import datetime
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict
import hashlib


@dataclass
class PromptExecution:
    """Tracks individual prompt execution with full attribution"""
    execution_id: str
    prompt_id: str
    prompt_name: str
    prompt_version: str
    content_type: str
    analyzer_type: str
    temperature: float
    model: str
    web_search_enabled: bool
    timestamp: datetime
    duration_seconds: float
    token_count: int
    parent_document_id: Optional[str] = None
    source_documents: List[Dict[str, str]] = None
    web_search_queries: List[str] = None
    
    def __post_init__(self):
        if self.source_documents is None:
            self.source_documents = []
        if self.web_search_queries is None:
            self.web_search_queries = []
    
    def to_notion_metadata(self) -> Dict[str, Any]:
        """Convert to Notion-compatible metadata format"""
        return {
            "execution_id": self.execution_id,
            "prompt_id": self.prompt_id,
            "prompt_name": self.prompt_name,
            "prompt_version": self.prompt_version,
            "timestamp": self.timestamp.isoformat(),
            "duration_seconds": self.duration_seconds,
            "model": self.model,
            "temperature": self.temperature,
            "web_search": self.web_search_enabled,
            "token_count": self.token_count
        }


class PromptAttributionSystem:
    """Main system for managing prompt attribution and traceability"""
    
    # Analyzer type to color mapping
    ANALYZER_COLORS = {
        "insights": "#8B5CF6",      # Purple
        "summarizer": "#3B82F6",    # Blue
        "classifier": "#10B981",    # Green
        "tagger": "#F59E0B",        # Yellow
        "technical": "#EF4444",     # Red
        "market": "#F97316",        # Orange
        "legal": "#92400E"          # Brown
    }
    
    # Analyzer type to icon mapping
    ANALYZER_ICONS = {
        "insights": "üìä",
        "summarizer": "üìù",
        "classifier": "üéØ",
        "tagger": "üè∑Ô∏è",
        "technical": "üîß",
        "market": "üìà",
        "legal": "‚öñÔ∏è"
    }
    
    def __init__(self, notion_client=None):
        self.notion_client = notion_client
        self.executions = {}
        
    def create_execution(self, prompt_config: Dict[str, Any]) -> PromptExecution:
        """Create a new prompt execution record"""
        execution = PromptExecution(
            execution_id=str(uuid.uuid4()),
            prompt_id=prompt_config['id'],
            prompt_name=prompt_config['name'],
            prompt_version=prompt_config.get('version', '1.0'),
            content_type=prompt_config['content_type'],
            analyzer_type=prompt_config['analyzer'],
            temperature=prompt_config['temperature'],
            model=prompt_config.get('model', 'claude-3-opus'),
            web_search_enabled=prompt_config.get('web_search', False),
            timestamp=datetime.utcnow(),
            duration_seconds=0.0,
            token_count=0
        )
        
        self.executions[execution.execution_id] = execution
        return execution
    
    def generate_attribution_header(self, execution: PromptExecution, quality_score: float = None) -> str:
        """Generate markdown attribution header for content"""
        icon = self.ANALYZER_ICONS.get(execution.analyzer_type, "ü§ñ")
        
        # Generate quality stars
        if quality_score:
            full_stars = int(quality_score)
            stars = "‚≠ê" * full_stars
            if quality_score % 1 >= 0.5:
                stars += "‚≠ê"
            quality_text = f"{stars} ({quality_score:.1f}/5)"
        else:
            quality_text = "Not yet rated"
        
        # Calculate time ago
        time_ago = self._format_time_ago(execution.timestamp)
        
        header = f"""
<div class="prompt-attribution" style="border-left: 4px solid {self.ANALYZER_COLORS.get(execution.analyzer_type, '#6B7280')}; padding-left: 16px; margin: 16px 0;">
  <div class="attribution-header" style="display: flex; align-items: center; gap: 12px; flex-wrap: wrap;">
    <span class="generated-by">
      {icon} Generated by: <a href="notion://page/{execution.prompt_id}">{execution.prompt_name}</a>
    </span>
    <span class="version-badge" style="background: #F3F4F6; padding: 2px 8px; border-radius: 4px; font-size: 12px;">
      v{execution.prompt_version}
    </span>
    <span class="quality-score">{quality_text}</span>
    <span class="last-updated" style="color: #6B7280; font-size: 12px;">Updated: {time_ago}</span>
  </div>
  <div class="attribution-metadata" style="margin-top: 8px; font-size: 12px; color: #6B7280;">
    Analyzer: {execution.analyzer_type} | Temperature: {execution.temperature} | Web Search: {"‚úÖ" if execution.web_search_enabled else "‚ùå"}
  </div>
  <div class="attribution-actions" style="margin-top: 12px; display: flex; gap: 8px;">
    <button onclick="viewPrompt('{execution.prompt_id}')" style="padding: 4px 12px; background: #F3F4F6; border: none; border-radius: 4px; cursor: pointer;">
      üìã View Prompt
    </button>
    <button onclick="rateOutput('{execution.execution_id}')" style="padding: 4px 12px; background: #F3F4F6; border: none; border-radius: 4px; cursor: pointer;">
      ‚≠ê Rate Output
    </button>
    <button onclick="suggestImprovement('{execution.prompt_id}')" style="padding: 4px 12px; background: #F3F4F6; border: none; border-radius: 4px; cursor: pointer;">
      üí° Suggest Improvement
    </button>
  </div>
</div>
"""
        return header
    
    def generate_inline_attribution(self, execution: PromptExecution, content: str) -> str:
        """Wrap content with inline attribution metadata"""
        metadata = {
            "prompt_id": execution.prompt_id,
            "prompt_name": execution.prompt_name,
            "temperature": execution.temperature,
            "model": execution.model,
            "timestamp": execution.timestamp.isoformat(),
            "duration": f"{execution.duration_seconds:.1f}s"
        }
        
        return f"""
<span class="ai-generated-content" 
      data-attribution='{json.dumps(metadata)}'
      title="Generated by {execution.prompt_name} at {execution.timestamp.strftime('%Y-%m-%d %H:%M')}">
  {content}
  <span class="attribution-icon" style="font-size: 12px; opacity: 0.5;">ü§ñ</span>
</span>
"""
    
    def create_notion_attribution_block(self, execution: PromptExecution, content: str) -> Dict:
        """Create a Notion block with attribution callout"""
        return {
            "object": "block",
            "type": "callout",
            "callout": {
                "rich_text": [{
                    "type": "text",
                    "text": {
                        "content": f"Generated by: {execution.prompt_name}",
                        "link": {"url": f"notion://page/{execution.prompt_id}"}
                    },
                    "annotations": {
                        "bold": False,
                        "italic": False,
                        "strikethrough": False,
                        "underline": False,
                        "code": False,
                        "color": "default"
                    }
                }],
                "icon": {"emoji": self.ANALYZER_ICONS.get(execution.analyzer_type, "ü§ñ")},
                "color": "blue_background"
            },
            "children": [
                {
                    "object": "block",
                    "type": "paragraph",
                    "paragraph": {
                        "rich_text": [{
                            "type": "text",
                            "text": {"content": content},
                            "annotations": {
                                "bold": False,
                                "italic": False,
                                "strikethrough": False,
                                "underline": False,
                                "code": False,
                                "color": "default"
                            }
                        }]
                    }
                }
            ]
        }
    
    def track_quality_metrics(self, execution_id: str, metrics: Dict[str, float]):
        """Track quality metrics for a prompt execution"""
        if execution_id in self.executions:
            execution = self.executions[execution_id]
            # Store metrics (in a real implementation, this would go to a database)
            execution.quality_metrics = metrics
    
    def generate_performance_dashboard(self, prompt_id: str) -> Dict[str, Any]:
        """Generate performance dashboard data for a specific prompt"""
        # Filter executions for this prompt
        prompt_executions = [
            exec for exec in self.executions.values() 
            if exec.prompt_id == prompt_id
        ]
        
        if not prompt_executions:
            return {"error": "No executions found for this prompt"}
        
        # Calculate metrics
        total_uses = len(prompt_executions)
        avg_duration = sum(e.duration_seconds for e in prompt_executions) / total_uses
        avg_tokens = sum(e.token_count for e in prompt_executions) / total_uses
        
        # Group by date
        daily_usage = {}
        for execution in prompt_executions:
            date_key = execution.timestamp.date().isoformat()
            if date_key not in daily_usage:
                daily_usage[date_key] = 0
            daily_usage[date_key] += 1
        
        return {
            "prompt_id": prompt_id,
            "prompt_name": prompt_executions[0].prompt_name if prompt_executions else "Unknown",
            "total_uses": total_uses,
            "average_duration": f"{avg_duration:.2f}s",
            "average_tokens": int(avg_tokens),
            "daily_usage": daily_usage,
            "analyzer_type": prompt_executions[0].analyzer_type if prompt_executions else "Unknown",
            "last_used": max(e.timestamp for e in prompt_executions).isoformat()
        }
    
    def suggest_alternative_prompts(self, current_prompt_id: str, content_type: str) -> List[Dict]:
        """Suggest alternative prompts based on content type and performance"""
        # In a real implementation, this would query the database
        # For now, return mock suggestions
        suggestions = [
            {
                "prompt_id": "alt-1",
                "name": f"{content_type}_Deep_Analysis_v3.0",
                "rating": 4.9,
                "strengths": ["Technical depth", "Citation accuracy"],
                "trade_offs": ["Slower generation (4.5s avg)"],
                "tag": "NEW!"
            },
            {
                "prompt_id": "alt-2",
                "name": f"{content_type}_Quick_Insights_v1.2",
                "rating": 4.3,
                "strengths": ["Speed (1.8s avg)", "Conciseness"],
                "trade_offs": ["Less comprehensive"],
                "tag": "FAST"
            }
        ]
        return suggestions
    
    def _format_time_ago(self, timestamp: datetime) -> str:
        """Format timestamp as human-readable time ago"""
        now = datetime.utcnow()
        delta = now - timestamp
        
        if delta.days > 0:
            return f"{delta.days} day{'s' if delta.days > 1 else ''} ago"
        elif delta.seconds > 3600:
            hours = delta.seconds // 3600
            return f"{hours} hour{'s' if hours > 1 else ''} ago"
        elif delta.seconds > 60:
            minutes = delta.seconds // 60
            return f"{minutes} minute{'s' if minutes > 1 else ''} ago"
        else:
            return "just now"


class PromptFeedbackCollector:
    """Handles user feedback collection for prompt quality"""
    
    def __init__(self):
        self.feedback_store = {}
    
    def submit_rating(self, execution_id: str, prompt_id: str, rating: int, 
                     feedback: str = None, user_id: str = None) -> Dict:
        """Submit user rating for a prompt execution"""
        feedback_entry = {
            "id": str(uuid.uuid4()),
            "execution_id": execution_id,
            "prompt_id": prompt_id,
            "rating": rating,
            "feedback": feedback,
            "user_id": user_id or "anonymous",
            "timestamp": datetime.utcnow().isoformat()
        }
        
        if prompt_id not in self.feedback_store:
            self.feedback_store[prompt_id] = []
        
        self.feedback_store[prompt_id].append(feedback_entry)
        return {"status": "success", "feedback_id": feedback_entry["id"]}
    
    def get_prompt_analytics(self, prompt_id: str) -> Dict:
        """Get analytics for a specific prompt"""
        if prompt_id not in self.feedback_store:
            return {"error": "No feedback found for this prompt"}
        
        feedbacks = self.feedback_store[prompt_id]
        ratings = [f["rating"] for f in feedbacks]
        
        # Calculate rating distribution
        rating_dist = {i: 0 for i in range(1, 6)}
        for rating in ratings:
            rating_dist[rating] += 1
        
        # Extract common feedback themes (in real implementation, use NLP)
        feedback_texts = [f["feedback"] for f in feedbacks if f["feedback"]]
        
        return {
            "prompt_id": prompt_id,
            "total_ratings": len(ratings),
            "average_rating": sum(ratings) / len(ratings) if ratings else 0,
            "rating_distribution": rating_dist,
            "recent_feedback": feedback_texts[-5:] if feedback_texts else [],
            "last_rated": feedbacks[-1]["timestamp"] if feedbacks else None
        }


# Example usage
if __name__ == "__main__":
    # Initialize attribution system
    attribution_system = PromptAttributionSystem()
    
    # Example prompt configuration
    prompt_config = {
        "id": "2366d7f5-23bc-8122-adbe-ead9c9223cf7",
        "name": "Market_News_Analyzer_v1.5",
        "version": "1.5",
        "content_type": "Market News",
        "analyzer": "market",
        "temperature": 0.4,
        "web_search": True
    }
    
    # Create execution
    execution = attribution_system.create_execution(prompt_config)
    
    # Simulate execution completion
    execution.duration_seconds = 2.8
    execution.token_count = 3521
    
    # Generate attribution header
    header = attribution_system.generate_attribution_header(execution, quality_score=4.2)
    print("Attribution Header:")
    print(header)
    
    # Generate performance dashboard
    dashboard = attribution_system.generate_performance_dashboard(execution.prompt_id)
    print("\nPerformance Dashboard:")
    print(json.dumps(dashboard, indent=2))
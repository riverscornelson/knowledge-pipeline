# Prompt Attribution System Integration Guide

## Quick Start Implementation

### 1. Add to Your Knowledge Pipeline

```python
# In your main pipeline file
from prompt_attribution_implementation import PromptAttributionSystem, PromptFeedbackCollector

# Initialize the attribution system
attribution_system = PromptAttributionSystem(notion_client=your_notion_client)
feedback_collector = PromptFeedbackCollector()

# When generating content
def generate_with_attribution(prompt_config, input_data):
    # Create execution record
    execution = attribution_system.create_execution(prompt_config)
    
    # Generate content (your existing logic)
    start_time = time.time()
    content = your_ai_generation_function(prompt_config, input_data)
    
    # Complete execution record
    execution.duration_seconds = time.time() - start_time
    execution.token_count = len(content.split()) * 1.3  # Rough estimate
    
    # Create attributed output
    attributed_content = attribution_system.generate_attribution_header(execution) + "\n\n" + content
    
    return attributed_content, execution
```

### 2. Add Frontend Components

```html
<!-- Include the CSS and JS files -->
<link rel="stylesheet" href="prompt-attribution-styles.css">
<script src="prompt-attribution-frontend.js"></script>

<!-- Your content with attribution -->
<div class="prompt-attribution insights">
  <div class="attribution-header">
    <span class="generated-by">
      üìä Generated by: <a href="#" data-action="view-prompt" data-prompt-id="2366d7f5-23bc-81ae-af2b-f449e1025bf2">Research_Insights_Strategic_v2.3</a>
    </span>
    <span class="version-badge">v2.3</span>
    <span class="quality-score">‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (4.8/5)</span>
    <span class="last-updated">Updated: 30 minutes ago</span>
  </div>
  <div class="attribution-metadata">
    Analyzer: insights | Temperature: 0.7 | Web Search: ‚úÖ
  </div>
  <div class="attribution-actions">
    <button data-action="view-prompt" data-prompt-id="2366d7f5-23bc-81ae-af2b-f449e1025bf2">üìã View Prompt</button>
    <button data-action="rate-output" data-execution-id="exec_123456">‚≠ê Rate Output</button>
    <button data-action="suggest-improvement" data-prompt-id="2366d7f5-23bc-81ae-af2b-f449e1025bf2">üí° Suggest Improvement</button>
  </div>
</div>
```

### 3. Update Notion Database Schema

Add these properties to your existing databases:

#### Knowledge Entries Database
```javascript
// Add relation to prompt database
"Generated_By_Prompt": {
  "type": "relation",
  "relation": {
    "database_id": "2366d7f5-23bc-8175-9eb0-c7449c8bdc9b"
  }
}

// Add execution metadata
"Execution_Metadata": {
  "type": "rich_text"  // Store JSON metadata
}

// Add quality score
"Quality_Score": {
  "type": "number",
  "number": {"format": "number"}
}
```

#### Prompt Database (Existing: 2366d7f5-23bc-8175-9eb0-c7449c8bdc9b)
```javascript
// Add performance rollups
"Total_Uses": {
  "type": "rollup",
  "rollup": {
    "relation_property": "Generated_By_Prompt",
    "rollup_property": "Created",
    "function": "count"
  }
}

"Average_Quality": {
  "type": "rollup",
  "rollup": {
    "relation_property": "Generated_By_Prompt", 
    "rollup_property": "Quality_Score",
    "function": "average"
  }
}
```

## API Endpoints

### 1. Feedback Submission

```python
# Add to your Flask/FastAPI app
@app.route('/api/prompt-attribution/feedback', methods=['POST'])
def submit_feedback():
    data = request.get_json()
    
    result = feedback_collector.submit_rating(
        execution_id=data['execution_id'],
        prompt_id=data.get('prompt_id'),
        rating=data['rating'],
        feedback=data.get('feedback'),
        user_id=data.get('user_id')
    )
    
    # Update Notion if needed
    if 'execution_id' in data:
        update_notion_quality_score(data['execution_id'], data['rating'])
    
    return jsonify(result)
```

### 2. Alternative Prompts

```python
@app.route('/api/prompt-attribution/alternatives')
def get_alternatives():
    prompt_id = request.args.get('prompt_id')
    content_type = request.args.get('content_type')
    
    alternatives = attribution_system.suggest_alternative_prompts(prompt_id, content_type)
    return jsonify(alternatives)
```

### 3. Performance Dashboard

```python
@app.route('/api/prompt-attribution/dashboard/<prompt_id>')
def get_dashboard(prompt_id):
    dashboard_data = attribution_system.generate_performance_dashboard(prompt_id)
    return jsonify(dashboard_data)
```

## Advanced Features

### 1. Real-time Quality Tracking

```python
class QualityTracker:
    def __init__(self):
        self.metrics_cache = {}
    
    def track_user_interaction(self, execution_id, interaction_type):
        """Track user interactions as quality signals"""
        metrics = self.metrics_cache.get(execution_id, {})
        
        if interaction_type == 'copy_content':
            metrics['copy_count'] = metrics.get('copy_count', 0) + 1
        elif interaction_type == 'edit_content':
            metrics['edit_count'] = metrics.get('edit_count', 0) + 1
        elif interaction_type == 'share_content':
            metrics['share_count'] = metrics.get('share_count', 0) + 1
        
        self.metrics_cache[execution_id] = metrics
        
        # Calculate implied quality score
        implied_quality = self.calculate_implied_quality(metrics)
        return implied_quality
    
    def calculate_implied_quality(self, metrics):
        """Calculate quality score from user behavior"""
        score = 3.0  # Base score
        
        # Positive signals
        score += metrics.get('copy_count', 0) * 0.2
        score += metrics.get('share_count', 0) * 0.3
        
        # Negative signals
        score -= metrics.get('edit_count', 0) * 0.1
        
        return min(5.0, max(1.0, score))
```

### 2. A/B Testing Framework

```python
class PromptABTesting:
    def __init__(self, attribution_system):
        self.attribution_system = attribution_system
        self.active_tests = {}
    
    def create_ab_test(self, prompt_a_id, prompt_b_id, content_type):
        """Create A/B test between two prompts"""
        test_id = str(uuid.uuid4())
        self.active_tests[test_id] = {
            'prompt_a': prompt_a_id,
            'prompt_b': prompt_b_id,
            'content_type': content_type,
            'results_a': [],
            'results_b': [],
            'created_at': datetime.utcnow()
        }
        return test_id
    
    def get_test_prompt(self, test_id, user_id):
        """Get prompt for user (consistent assignment)"""
        if test_id not in self.active_tests:
            return None
        
        # Consistent assignment based on user_id hash
        user_hash = hashlib.md5(str(user_id).encode()).hexdigest()
        use_prompt_a = int(user_hash, 16) % 2 == 0
        
        test = self.active_tests[test_id]
        return test['prompt_a'] if use_prompt_a else test['prompt_b']
    
    def record_result(self, test_id, prompt_id, quality_score):
        """Record A/B test result"""
        if test_id not in self.active_tests:
            return
        
        test = self.active_tests[test_id]
        if prompt_id == test['prompt_a']:
            test['results_a'].append(quality_score)
        elif prompt_id == test['prompt_b']:
            test['results_b'].append(quality_score)
    
    def get_test_results(self, test_id):
        """Get A/B test statistical results"""
        if test_id not in self.active_tests:
            return None
        
        test = self.active_tests[test_id]
        results_a = test['results_a']
        results_b = test['results_b']
        
        if len(results_a) < 10 or len(results_b) < 10:
            return {'status': 'insufficient_data'}
        
        avg_a = sum(results_a) / len(results_a)
        avg_b = sum(results_b) / len(results_b)
        
        # Simple significance test (use scipy.stats.ttest_ind for real implementation)
        difference = abs(avg_a - avg_b)
        significant = difference > 0.2  # Simplified threshold
        
        return {
            'status': 'complete',
            'prompt_a_avg': avg_a,
            'prompt_b_avg': avg_b,
            'difference': difference,
            'significant': significant,
            'winner': test['prompt_a'] if avg_a > avg_b else test['prompt_b'],
            'sample_sizes': {'a': len(results_a), 'b': len(results_b)}
        }
```

### 3. Automated Quality Monitoring

```python
class QualityMonitor:
    def __init__(self, attribution_system, threshold=3.5):
        self.attribution_system = attribution_system
        self.quality_threshold = threshold
        self.alerts_sent = set()
    
    def monitor_prompt_quality(self, prompt_id):
        """Monitor prompt quality and send alerts if degraded"""
        analytics = feedback_collector.get_prompt_analytics(prompt_id)
        
        if 'average_rating' not in analytics:
            return
        
        avg_rating = analytics['average_rating']
        total_ratings = analytics['total_ratings']
        
        # Only alert if we have sufficient data
        if total_ratings >= 10 and avg_rating < self.quality_threshold:
            alert_key = f"{prompt_id}_{avg_rating:.1f}"
            
            if alert_key not in self.alerts_sent:
                self.send_quality_alert(prompt_id, avg_rating, analytics)
                self.alerts_sent.add(alert_key)
    
    def send_quality_alert(self, prompt_id, avg_rating, analytics):
        """Send quality degradation alert"""
        # Get prompt details from Notion
        prompt_details = self.get_prompt_details(prompt_id)
        
        alert_message = f"""
        üö® Prompt Quality Alert
        
        Prompt: {prompt_details.get('name', 'Unknown')}
        Current Rating: {avg_rating:.1f}/5.0
        Total Ratings: {analytics['total_ratings']}
        
        Recent Feedback:
        {chr(10).join(analytics.get('recent_feedback', [])[:3])}
        
        Action Required: Review and update prompt
        """
        
        # Send to monitoring system (Slack, email, etc.)
        self.send_alert_notification(alert_message)
```

## Notion Integration Examples

### 1. Create Knowledge Entry with Attribution

```python
def create_knowledge_entry_with_attribution(title, content, execution):
    """Create Notion page with full attribution"""
    
    # Create the main content blocks
    content_blocks = [
        # Attribution header
        attribution_system.create_notion_attribution_block(execution, ""),
        
        # Main content
        {
            "object": "block",
            "type": "paragraph",
            "paragraph": {
                "rich_text": [{
                    "type": "text",
                    "text": {"content": content}
                }]
            }
        }
    ]
    
    # Create page with properties
    new_page = notion_client.pages.create(
        parent={"database_id": KNOWLEDGE_DATABASE_ID},
        properties={
            "Title": {
                "title": [{"text": {"content": title}}]
            },
            "Generated_By_Prompt": {
                "relation": [{"id": execution.prompt_id}]
            },
            "Execution_Metadata": {
                "rich_text": [{
                    "text": {"content": json.dumps(execution.to_notion_metadata())}
                }]
            },
            "Quality_Score": {
                "number": 0  # Will be updated when users rate
            }
        },
        children=content_blocks
    )
    
    return new_page
```

### 2. Update Prompt Performance Metrics

```python
def update_prompt_performance(prompt_id):
    """Update rollup properties and performance metrics"""
    
    # Get all executions for this prompt
    executions = get_executions_for_prompt(prompt_id)
    
    # Calculate metrics
    total_uses = len(executions)
    avg_duration = sum(e.duration_seconds for e in executions) / total_uses if executions else 0
    success_rate = len([e for e in executions if e.success]) / total_uses if executions else 0
    
    # Update prompt page
    notion_client.pages.update(
        page_id=prompt_id,
        properties={
            "Performance_Metrics": {
                "rich_text": [{
                    "text": {"content": json.dumps({
                        "total_uses": total_uses,
                        "avg_duration": round(avg_duration, 2),
                        "success_rate": round(success_rate * 100, 1),
                        "last_updated": datetime.utcnow().isoformat()
                    })}
                }]
            }
        }
    )
```

## Testing the Integration

### 1. Unit Tests

```python
import unittest
from prompt_attribution_implementation import PromptAttributionSystem

class TestPromptAttribution(unittest.TestCase):
    def setUp(self):
        self.attribution_system = PromptAttributionSystem()
        self.sample_prompt_config = {
            "id": "test-prompt-123",
            "name": "Test_Analyzer_v1.0",
            "version": "1.0",
            "content_type": "Test",
            "analyzer": "insights",
            "temperature": 0.5,
            "web_search": True
        }
    
    def test_create_execution(self):
        execution = self.attribution_system.create_execution(self.sample_prompt_config)
        
        self.assertEqual(execution.prompt_id, "test-prompt-123")
        self.assertEqual(execution.prompt_name, "Test_Analyzer_v1.0")
        self.assertEqual(execution.temperature, 0.5)
        self.assertTrue(execution.web_search_enabled)
    
    def test_attribution_header_generation(self):
        execution = self.attribution_system.create_execution(self.sample_prompt_config)
        header = self.attribution_system.generate_attribution_header(execution, 4.5)
        
        self.assertIn("Test_Analyzer_v1.0", header)
        self.assertIn("‚≠ê‚≠ê‚≠ê‚≠ê", header)
        self.assertIn("insights", header)
    
    def test_notion_block_creation(self):
        execution = self.attribution_system.create_execution(self.sample_prompt_config)
        block = self.attribution_system.create_notion_attribution_block(execution, "Test content")
        
        self.assertEqual(block["type"], "callout")
        self.assertIn("Test_Analyzer_v1.0", block["callout"]["rich_text"][0]["text"]["content"])
```

### 2. Integration Test

```python
def test_end_to_end_attribution():
    """Test complete attribution flow"""
    
    # 1. Create execution
    prompt_config = get_test_prompt_config()
    execution = attribution_system.create_execution(prompt_config)
    
    # 2. Generate content (mock)
    content = "This is test generated content with insights..."
    execution.duration_seconds = 2.5
    execution.token_count = 150
    
    # 3. Create attribution header
    header = attribution_system.generate_attribution_header(execution, 4.2)
    
    # 4. Submit feedback
    feedback_result = feedback_collector.submit_rating(
        execution_id=execution.execution_id,
        prompt_id=execution.prompt_id,
        rating=4,
        feedback="Good analysis but could be more specific"
    )
    
    # 5. Get performance dashboard
    dashboard = attribution_system.generate_performance_dashboard(execution.prompt_id)
    
    # Verify all components work together
    assert header is not None
    assert feedback_result["status"] == "success"
    assert dashboard["total_uses"] >= 1
    
    print("‚úÖ End-to-end attribution test passed!")
```

## Deployment Checklist

- [ ] Add CSS file to your static assets
- [ ] Include JavaScript file in your templates
- [ ] Update Notion database schema with new properties
- [ ] Implement API endpoints for feedback and alternatives
- [ ] Add attribution headers to existing content generation
- [ ] Set up quality monitoring alerts
- [ ] Test on mobile devices
- [ ] Configure analytics tracking
- [ ] Train team on new attribution features
- [ ] Update documentation for users

## Performance Considerations

1. **Caching**: Cache prompt configurations and performance metrics
2. **Async Updates**: Update quality scores asynchronously to avoid blocking content generation
3. **Database Indexing**: Index execution_id and prompt_id for fast lookups
4. **Rate Limiting**: Limit feedback submission to prevent spam
5. **Lazy Loading**: Load attribution details on demand for large documents

This integration guide provides everything needed to implement the complete prompt attribution and traceability system in your knowledge pipeline.
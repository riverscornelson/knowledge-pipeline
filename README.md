# Knowledge Pipeline

Production knowledge pipeline that captures content from multiple sources (PDFs, websites, Gmail) and enriches it with AI-generated summaries and classifications. The system uses OpenAI models via the Responses API with automatic fallback to Chat Completions.

Includes an AI-powered daily newsletter system that generates cross-analysis of enriched content.

## Utilities

| Script | Description |
|--------|-------------|
| **üöÄ PRODUCTION PIPELINE** |
| `pipeline_consolidated.sh` | **Main pipeline**: Runs ingest ‚Üí capture ‚Üí enrich with 75% performance improvement |
| **üì• CONTENT INGESTION** |
| `ingest_drive.py` | Scan Google Drive folder for new PDFs and create pages in Sources database |
| `capture_websites.py` | Scrape websites using Firecrawl API with authentication support |
| `capture_emails.py` | Capture newsletter emails from Gmail using OAuth2 (‚ö†Ô∏è has Source Type bug) |
| **üß† AI ENRICHMENT** |
| `enrich_consolidated.py` | **Unified enrichment**: Replaces 7 legacy scripts with streamlined AI analysis |
| **üì∞ NEWSLETTER SYSTEM** |
| `daily_newsletter.py` | Generate and send AI-powered daily newsletter with cross-analysis |
| `test_newsletter.py` | Test newsletter generation with mock data |
| `test_email_content.py` | Test email content formatting |
| **üîß UTILITIES** |
| `infer_vendor.py` | Backfill vendor fields on existing pages |
| `infer_created_date.py` | Populate missing Created Date using article text |
| `migration_v2.py` | Database migration and backup tools |

## Setup

1. *(Optional)* Create and activate a virtual environment.
2. Install dependencies:

```bash
pip install -r requirements.txt
```

3. Provide configuration via environment variables or a `.env` file.
4. *(Optional)* For website scraping with authentication, copy `auth_configs.json.example` to `auth_configs.json` and configure your credentials.

### Environment variables

| Variable | Purpose |
|----------|---------|
| `NOTION_TOKEN` | Notion integration token |
| `NOTION_SOURCES_DB` | ID of the target Notion database |
| `GOOGLE_APP_CREDENTIALS` | Path to a Google service-account JSON key |
| `OPENAI_API_KEY` | OpenAI API key |
| `MODEL_SUMMARY` | Model used for summary generation (default `gpt-4.1`) |
| `MODEL_CLASSIFIER` | Model for classification (default `gpt-4.1`) |
| `MODEL_POSTPROCESS` | Model for additional prompts (default `gpt-4.1`) |
| `MODEL_VENDOR` | Model for vendor inference (default `gpt-4.1`) |
| `DRIVE_FOLDER_ID` | Google Drive folder ID for `ingest_drive.py` (optional) |
| `RSS_FEEDS` | Comma-separated RSS or Substack URLs for `capture_rss.py` |
| `RSS_URL_PROP` | Property name for the article URL (default `Article URL`) |
| `CREATED_PROP` | Property name for the created date (default `Created Date`) |
| **Website Scraping** | |
| `FIRECRAWL_API_KEY` | Firecrawl API key for website scraping |
| `WEBSITE_SOURCES` | Comma-separated URLs to scrape |
| `WEBSITE_AUTH_CONFIGS` | Path to auth config JSON file (optional) |
| `WEBSITE_WINDOW_DAYS` | Days of recency for website content (default `30`) |
| **Email Integration** | |
| `GMAIL_CREDENTIALS_PATH` | Path to Gmail OAuth2 credentials JSON (default `gmail_credentials/credentials.json`) |
| `GMAIL_TOKEN_PATH` | Path to stored Gmail token (default `gmail_credentials/token.json`) |
| `GMAIL_SEARCH_QUERY` | Gmail search query (default `from:newsletter OR from:substack`) |
| `GMAIL_WINDOW_DAYS` | Days to look back for emails (default `7`) |
| **Daily Newsletter** | |
| `NEWSLETTER_RECIPIENTS` | Comma-separated email addresses for newsletter delivery |
| `NEWSLETTER_SENDER_NAME` | Display name for newsletter sender (default "Knowledge Pipeline") |

Ensure your database includes a **Created Date** property.

## Sources Database

All scripts operate on a Notion table named **Sources**. Each entry
represents either a PDF from Drive, an online article captured from an RSS
feed, or website content scraped with Firecrawl. The table requires a number of properties so the utilities can track and
enrich the content:

| Property | Type | Purpose |
|----------|------|---------|
| **Title** | Title | Primary title of the document or article. Set when the entry is created. |
| **AI-Primitive** | Multi-select | Category of AI capability inferred during enrichment. |
| **Content-Type** | Select | Overall type of content (e.g. Market News, Vendor Capability). |
| **Drive URL** | URL | Link to the PDF on Google Drive. Present only for PDF sources. |
| **Article URL** | URL | Link to the web article for RSS entries and website sources. |
| **Hash** | Text | SHA‚Äë256 hash used to detect duplicates. |
| **Status** | Select | Workflow state (`Inbox`, `Enriched`, `Failed`). |
| **Summary** | Text | Short executive summary generated by the enrichment step. |
| **Vendor** | Select | Vendor or organisation name if applicable. |
| **Created Date** | Date | Original publication or file creation time. |
| **Source Type** | Select | ‚ö†Ô∏è **ISSUE**: Referenced in documentation but missing from database. |

The values for **AI-Primitive** and **Content-Type** are fetched from the
database schema each time the scripts run. Updating the select options in
Notion automatically changes the classification taxonomy.

For each page, the enrichment process also appends toggle blocks containing a
detailed summary, the raw extracted text and outputs from additional analysis
prompts. These blocks live in the page body so that readers can expand them in
Notion for a deeper dive into the content.

## Usage

### üöÄ Production Pipeline (Recommended)

Run the complete consolidated pipeline:

```bash
# Main pipeline (75% faster than legacy approach)
./pipeline_consolidated.sh

# Optional: Generate daily newsletter after pipeline completes
python daily_newsletter.py
```

The consolidated pipeline automatically runs:
1. PDF ingestion from Google Drive
2. Website scraping with Firecrawl
3. Gmail newsletter capture
4. Unified AI enrichment (3 calls vs 20+ in legacy)

### üìä Individual Scripts (Advanced)

For debugging or selective processing:

```bash
# Content ingestion
python ingest_drive.py        # PDFs from Google Drive
python capture_websites.py   # Websites via Firecrawl
python capture_emails.py     # Gmail newsletters (‚ö†Ô∏è has bug)

# AI enrichment (replaces 7 legacy scripts)
python enrich_consolidated.py

# Newsletter system
python daily_newsletter.py   # Generate and send newsletter
python test_newsletter.py    # Test with mock data

# Utilities
python infer_vendor.py        # Backfill vendor fields
python infer_created_date.py  # Backfill creation dates
```

### ‚ö†Ô∏è Known Issues

- **Email capture**: `capture_emails.py` fails due to missing "Source Type" property in Notion database
- **The Verge scraping**: Occasional timeouts when scraping theverge.com

---

These utilities rely on `python-dotenv`, `google-api-python-client`, `notion-client` and the OpenAI API. Any `.idea/` directory can be ignored.

# Knowledge Pipeline

Small utilities for capturing PDFs and RSS articles and storing them in a Notion database. The scripts can summarise and classify content using OpenAI models via the Responses API.
They automatically fall back to Chat Completions if the installed OpenAI client
doesn't yet support the Responses API (version < 1.3).
The Responses API is preferred across the pipeline for all OpenAI calls.

## Utilities

| Script | Description |
|--------|-------------|
| `ingest_drive.py` | Scan a Google Drive folder for new PDFs and create pages in the **Sources** database. Existing files are skipped without re-downloading. |
| `enrich.py` | Download PDFs referenced in the database, extract text, generate summaries, classify topics and update each page. |
| `capture_rss.py` | Pull new entries from RSS feeds or Substack newsletters and add them to the database. |
| `capture_websites.py` | **NEW**: Scrape websites using Firecrawl API with authentication support for paid content. |
| `enrich_rss.py` | Summarise and classify RSS articles and website content already stored in Notion. |
| `postprocess.py` | Apply additional enrichment prompts. Used by `enrich.py` and `enrich_rss.py`. |
| `infer_vendor.py` | Infer and set the Vendor field on existing pages. |
| `infer_created_date.py` | Populate missing Created Date using article text. |

## Setup

1. *(Optional)* Create and activate a virtual environment.
2. Install dependencies:

```bash
pip install -r requirements.txt
```

3. Provide configuration via environment variables or a `.env` file.
4. *(Optional)* For website scraping with authentication, copy `auth_configs.json.example` to `auth_configs.json` and configure your credentials.

### Environment variables

| Variable | Purpose |
|----------|---------|
| `NOTION_TOKEN` | Notion integration token |
| `NOTION_SOURCES_DB` | ID of the target Notion database |
| `GOOGLE_APP_CREDENTIALS` | Path to a Google service-account JSON key |
| `OPENAI_API_KEY` | OpenAI API key |
| `MODEL_SUMMARY` | Model used for summary generation (default `gpt-4.1`) |
| `MODEL_CLASSIFIER` | Model for classification (default `gpt-4.1`) |
| `MODEL_POSTPROCESS` | Model for additional prompts (default `gpt-4.1`) |
| `MODEL_VENDOR` | Model for vendor inference (default `gpt-4.1`) |
| `DRIVE_FOLDER_ID` | Google Drive folder ID for `ingest_drive.py` (optional) |
| `RSS_FEEDS` | Comma-separated RSS or Substack URLs for `capture_rss.py` |
| `RSS_URL_PROP` | Property name for the article URL (default `Article URL`) |
| `CREATED_PROP` | Property name for the created date (default `Created Date`) |
| `RSS_WINDOW_DAYS` | Days of recency for RSS items (default `90`) |
| **Website Scraping** | |
| `FIRECRAWL_API_KEY` | **NEW**: Firecrawl API key for website scraping |
| `WEBSITE_SOURCES` | **NEW**: Comma-separated URLs to scrape |
| `WEBSITE_AUTH_CONFIGS` | **NEW**: Path to auth config JSON file (optional) |
| `WEBSITE_WINDOW_DAYS` | **NEW**: Days of recency for website content (default `30`) |

Ensure your database includes a **Created Date** property.

## Sources Database

All scripts operate on a Notion table named **Sources**. Each entry
represents either a PDF from Drive, an online article captured from an RSS
feed, or website content scraped with Firecrawl. The table requires a number of properties so the utilities can track and
enrich the content:

| Property | Type | Purpose |
|----------|------|---------|
| **Title** | Title | Primary title of the document or article. Set when the entry is created. |
| **AI-Primitive** | Multi-select | Category of AI capability inferred during enrichment. |
| **Content-Type** | Select | Overall type of content (e.g. Market News, Vendor Capability). |
| **Drive URL** | URL | Link to the PDF on Google Drive. Present only for PDF sources. |
| **Article URL** | URL | Link to the web article for RSS entries and website sources. |
| **Hash** | Text | SHAâ€‘256 hash used to detect duplicates. |
| **Status** | Select | Workflow state (`Inbox`, `Enriched`, `Failed`). |
| **Summary** | Text | Short executive summary generated by the enrichment step. |
| **Vendor** | Select | Vendor or organisation name if applicable. |
| **Created Date** | Date | Original publication or file creation time. |
| **Source Type** | Select | **NEW**: Type of source (`RSS`, `PDF`, `Website`). |

The values for **AI-Primitive** and **Content-Type** are fetched from the
database schema each time the scripts run. Updating the select options in
Notion automatically changes the classification taxonomy.

For each page, the enrichment process also appends toggle blocks containing a
detailed summary, the raw extracted text and outputs from additional analysis
prompts. These blocks live in the page body so that readers can expand them in
Notion for a deeper dive into the content.

## Usage

1. Ingest new PDFs from Drive:

```bash
python ingest_drive.py
```

2. Summarise and classify them:

```bash
python enrich.py
```

3. Summarise existing RSS articles:

```bash
python enrich_rss.py
```

Both enrichment scripts automatically call `postprocess.py` to add extra
analysis blocks after the initial summary and classification.

4. Capture new items from RSS feeds:

```bash
python capture_rss.py
```

If a feed URL points to a Substack homepage, `capture_rss.py` automatically appends `/feed`. Use `RSS_URL_PROP` if your database stores article URLs under a different property name.

5. **NEW**: Scrape websites with Firecrawl:

```bash
python capture_websites.py
```

This supports authenticated scraping for paid Substacks and other premium content using the auth config file.

6. Populate missing Vendor fields:

```bash
python infer_vendor.py
```

7. Backfill missing Created Dates:

```bash
python infer_created_date.py
```

Items older than `RSS_WINDOW_DAYS` are ignored when capturing RSS feeds.

---

These utilities rely on `python-dotenv`, `google-api-python-client`, `notion-client` and the OpenAI API. Any `.idea/` directory can be ignored.

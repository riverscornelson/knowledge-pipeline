"""
enrich.py  â€“ Drive PDF âœ Notion enrichment
  â€¢ Summary (â‰ˆ250 words) with GPT-4.1
  â€¢ Classification with GPT-4.1

ENV (.env)
  NOTION_TOKEN, NOTION_SOURCES_DB
  GOOGLE_APP_CREDENTIALS
  OPENAI_API_KEY
  MODEL_SUMMARY=gpt-4.1
  MODEL_CLASSIFIER=gpt-4.1
"""
import os, io, json, re, time
from dotenv import load_dotenv
from notion_client import Client as Notion
from googleapiclient.discovery import build
from google.oauth2.service_account import Credentials
from googleapiclient.http import MediaIoBaseDownload
from pdfminer.high_level import extract_text
from tenacity import retry, wait_exponential, stop_after_attempt
from openai import OpenAI, APIError, RateLimitError
import openai
from postprocess import post_process_page
from infer_vendor import infer_vendor_name

# â”€â”€ init â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()
NOTION_DB        = os.getenv("NOTION_SOURCES_DB")
MODEL_SUMMARY    = os.getenv("MODEL_SUMMARY",    "gpt-4.1")
MODEL_CLASSIFIER = os.getenv("MODEL_CLASSIFIER", "gpt-4.1")

notion = Notion(auth=os.getenv("NOTION_TOKEN"))
oai    = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

HAS_RESPONSES = hasattr(oai, "responses")

def _chat_create(**kwargs):
    """Compatibility wrapper for ChatCompletion calls."""
    if hasattr(oai, "chat"):
        return oai.chat.completions.create(**kwargs)
    # Fallback for very old openai versions
    openai.api_key = os.getenv("OPENAI_API_KEY")
    return openai.ChatCompletion.create(**kwargs)

drive = build(
    "drive", "v3",
    credentials=Credentials.from_service_account_file(os.getenv("GOOGLE_APP_CREDENTIALS")),
    cache_discovery=False
)

# â”€â”€ helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

MAX_CHUNK = 1900                    # Notion limit per text object

def add_fulltext_blocks(page_id: str, full_text: str):
    """Append the extracted text as a toggle block containing paragraph chunks."""
    chunks = [full_text[i:i+MAX_CHUNK] for i in range(0, len(full_text), MAX_CHUNK)]
    # limit to first 25 chunks (â‰ˆ50k) â€“ adjust as you wish
    chunks = chunks[:25]

    children = [
        {
            "object": "block",
            "type": "paragraph",
            "paragraph": {
                "rich_text": [
                    {"type": "text", "text": {"content": chunk}}
                ]
            },
        }
        for chunk in chunks
    ]

    block = {
        "object": "block",
        "type": "toggle",
        "toggle": {
            "rich_text": [{"type": "text", "text": {"content": "Raw"}}],
            "children": children,
        },
    }

    notion.blocks.children.append(page_id, children=[block])


def add_summary_block(page_id: str, summary: str):
    """Append a toggle block containing the provided summary."""
    block = {
        "object": "block",
        "type": "toggle",
        "toggle": {
            "rich_text": [{"type": "text", "text": {"content": "Summary"}}],
            "children": [
                {
                    "object": "block",
                    "type": "paragraph",
                    "paragraph": {
                        "rich_text": [
                            {
                                "type": "text",
                                "text": {"content": summary[:MAX_CHUNK]}
                            }
                        ]
                    },
                }
            ],
        },
    }

    notion.blocks.children.append(page_id, children=[block])


def inbox_rows(require_url: str | None = None):
    """Return all rows in the Inbox, optionally ensuring a URL property is set."""
    base_filter = {"property": "Status", "select": {"equals": "Inbox"}}
    if require_url:
        filter_ = {"and": [base_filter,
                              {"property": require_url,
                               "url": {"is_not_empty": True}}]}
    else:
        filter_ = base_filter

    results = []
    kwargs = dict(database_id=NOTION_DB, filter=filter_, page_size=100)
    while True:
        resp = notion.databases.query(**kwargs)
        results.extend(resp["results"])
        if not resp.get("has_more"):
            break
        kwargs["start_cursor"] = resp["next_cursor"]
    return results

def drive_id(url: str) -> str:
    """Extract the file ID from a Google Drive share URL."""
    try:
        return url.split("/d/")[1].split("/")[0]
    except Exception:
        raise ValueError(f"invalid Drive URL: {url}")

def download_pdf(fid: str) -> bytes:
    buf = io.BytesIO()
    MediaIoBaseDownload(buf, drive.files().get_media(fileId=fid)).next_chunk()
    return buf.getvalue()

@retry(wait=wait_exponential(2, 30), stop=stop_after_attempt(5),
       retry=lambda e: isinstance(e, (APIError, RateLimitError)))
def summarise(text: str) -> str:
    """Summarise text using the Responses API."""
    text = text[:10000000000]
    instructions = (
        "Summarise the following document in no more than 175 words, markdown. "
        "Focus on relevant takeaways for AI use in business."
    )
    if HAS_RESPONSES:
        resp = oai.responses.create(
            model=MODEL_SUMMARY,
            instructions=instructions,
            input=text,
            max_output_tokens=1000,
        )
        out = resp.output[0]
        return out.content[0].text.strip()
    else:
        resp = _chat_create(
            model=MODEL_SUMMARY,
            messages=[{"role": "system", "content": instructions},
                      {"role": "user", "content": text}],
            max_tokens=1000,
        )
        return resp.choices[0].message.content.strip()

@retry(wait=wait_exponential(2, 30), stop=stop_after_attempt(5),
       retry=lambda e: isinstance(e, (APIError, RateLimitError)))
def summarise_exec(text: str) -> str:
    """Return a five-sentence executive summary using the Responses API."""
    text = text[:10000000000]
    instructions = (
        "Summarise the following document in five sentences for an "
        "executive audience. Focus on relevant takeaways for AI use in business."
    )
    if HAS_RESPONSES:
        resp = oai.responses.create(
            model=MODEL_SUMMARY,
            instructions=instructions,
            input=text,
            max_output_tokens=1000,
        )
        out = resp.output[0]
        return out.content[0].text.strip()
    else:
        resp = _chat_create(
            model=MODEL_SUMMARY,
            messages=[{"role": "system", "content": instructions},
                      {"role": "user", "content": text}],
            max_tokens=1000,
        )
        return resp.choices[0].message.content.strip()

@retry(wait=wait_exponential(2, 30), stop=stop_after_attempt(5),
       retry=lambda e: isinstance(e, (APIError, RateLimitError)))
def classify(text: str) -> tuple[str, str]:
    """Return a (content_type, ai_primitive) tuple from the text."""

    allowed_ct = {
        "Market News",
        "Thought Leadership",
        "Personal Note",
        "Vendor Capability",
        "Client Deliverable",
    }

    allowed_ap = {
        "Content Creation",
        "Research",
        "Coding",
        "Data Analysis",
        "Ideation/Strategy",
        "Automation",
    }

    instr = (
        "Classify the document and respond with JSON in the format\n"
        "{\n  \"content_type\": <one of: %s>,\n  \"ai_primitive\": <one of: %s>\n}"
        % (", ".join(sorted(allowed_ct)), ", ".join(sorted(allowed_ap)))
        + ". If unsure, choose the closest match."
    )

    txt = text[:600000]
    if HAS_RESPONSES:
        resp = oai.responses.create(
            model=MODEL_CLASSIFIER,
            instructions=instr,
            input=txt,
            max_output_tokens=60,
        )
        out = resp.output[0].content[0].text.strip()
    else:
        resp = _chat_create(
            model=MODEL_CLASSIFIER,
            messages=[{"role": "system", "content": instr},
                      {"role": "user", "content": txt}],
            max_tokens=60,
        )
        out = resp.choices[0].message.content.strip()

    m = re.search(r"\{.*\}", out, re.S)
    if not m:
        raise ValueError("GPT-4.1 did not return JSON")
    try:
        args = json.loads(m.group(0))
    except json.JSONDecodeError as exc:
        raise ValueError(f"invalid JSON: {exc}")

    if args.get("content_type") not in allowed_ct:
        print("   âš ï¸  Invalid content_type ->", args.get("content_type"))
        args["content_type"] = "Thought Leadership"
    if args.get("ai_primitive") not in allowed_ap:
        print("   âš ï¸  Invalid ai_primitive ->", args.get("ai_primitive"))
        args["ai_primitive"] = "Research"

    return args["content_type"], args["ai_primitive"]

def notion_update(pid, status, summary=None, ctype=None, prim=None, vendor=None):
    """Update a Notion page status and optional five-sentence summary."""
    props = {"Status": {"select": {"name": status}}}

    if summary is not None:
        content = summary.strip() or "âš ï¸ summary empty"
        props["Summary"] = {
            "rich_text": [{"type": "text", "text": {"content": content[:1950]}}]
        }
    if ctype:
        props["Content-Type"] = {"select": {"name": ctype}}
    if prim:
        props["AI-Primitive"] = {"multi_select": [{"name": prim}]}
    if vendor:
        props["Vendor"] = {"select": {"name": vendor}}

    notion.pages.update(pid, properties=props)

# â”€â”€ main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    rows = inbox_rows(require_url="Drive URL")
    if not rows:
        print("ğŸš© Nothing in Inbox."); return
    print(f"ğŸ” Found {len(rows)} row(s) to enrich\n")

    for row in rows:
        title = row["properties"]["Title"]["title"][0]["plain_text"]
        print(f"â¡ï¸  {title}")

        drive_prop = row["properties"].get("Drive URL")
        url = drive_prop.get("url") if drive_prop else None

        try:
            fid = drive_id(url)
            print("   â€¢ Downloading â€¦")

            pdf_text = extract_text(io.BytesIO(download_pdf(fid)))

            print("   â€¢ Summarising with GPT-4.1 â€¦")
            detail = summarise(pdf_text)
            add_summary_block(row["id"], detail)

            add_fulltext_blocks(row["id"], pdf_text)

            print("     â†³ extracted chars:", len(pdf_text))

            if not pdf_text.strip():
                raise ValueError("empty text after extraction")

            print("   â€¢ Summarising with GPT-4.1 (exec) â€¦")
            summary = summarise_exec(pdf_text)
            print("     â†³ exec summary chars:", len(summary))

            print("   â€¢ Classifying with GPT-4.1 â€¦")
            ctype, prim = classify(pdf_text)
            print(f"     â†³ {ctype}  /  {prim}")

            vendor = None
            vend_prop = row["properties"].get("Vendor", {})
            if not vend_prop.get("select"):
                print("   â€¢ Inferring vendor â€¦")
                try:
                    vendor = infer_vendor_name(summary or pdf_text)
                    if vendor == "Unknown":
                        print("     â†³ Vendor: Unknown")
                        vendor = None
                    else:
                        print(f"     â†³ Vendor: {vendor}")
                except Exception as exc:
                    print(f"     âš ï¸ Vendor inference error: {exc}")

            print("   â€¢ Post-processing â€¦")
            post_process_page(row["id"], pdf_text)

            notion_update(row["id"], "Enriched", summary, ctype, prim, vendor)
            print("âœ… Updated row â†’ Enriched\n")

        except Exception as err:
            print("âŒ", err, "\n")
            notion_update(row["id"], "Failed")
        time.sleep(0.3)

if __name__ == "__main__":
    main()
